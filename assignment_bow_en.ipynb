{"nbformat":4,"nbformat_minor":5,"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6e6e5e3f"},"source":["# Workshop - Summarizing text extractively\n","\n","In this task, we will examine a classic application of TF-IDF for extractive text summarization.\n","\n","## Document retrieval"],"id":"6e6e5e3f"},{"cell_type":"markdown","metadata":{"id":"ebae5a44"},"source":["\n","## Text summarization\n","\n","Text summarization is a typical natural language processing task that aims to extract relevant information from a given text. There are two main approaches to this task:\n","\n","* **Extractive text summarization**: This task aims to retrieve the most relevant chunks of text that are most likely to summarize the content of the text. In this task, textual chunks, sections, or segments of the text are obtained. For example:\n","> Input: \"Alan Mathison Turing OBE FRS (/ˈtjʊərɪŋ/; June 23, 1912 - June 7, 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher, and theoretical biologist. [6][7] Turing had a major influence on the development of theoretical computer science, providing a formalization of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer.[8][9][10] Turing is widely regarded as the father of theoretical computer science and artificial intelligence.[11]\"\n","\n","  > Output: “Alan Mathison Turing OBE FRS (/ˈtjʊərɪŋ/; June 23, 1912 – June 7, 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher, and theoretical biologist.”\n","* **Abstract text summarization**: This is a task that aims to synthesize the text, i.e., when the summary does not necessarily have to be part of the text. It involves the automatic generation of a coherent and related text."],"id":"ebae5a44"},{"cell_type":"markdown","metadata":{"id":"918c8d00"},"source":["## Required libraries\n","\n","This task must be resolved with the following dependencies:**"],"id":"918c8d00"},{"cell_type":"code","metadata":{"id":"0a8c3d8c"},"source":["import re\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import spacy\n","\n","from wordcloud import WordCloud\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"id":"0a8c3d8c","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cde7bf5f"},"source":["## Data\n","\n","Let's define the text we are going to process:"],"id":"cde7bf5f"},{"cell_type":"code","metadata":{"lines_to_next_cell":0,"id":"d41ac8d3"},"source":["text = \"\"\"Geoffrey Everest Hinton CC FRS FRSC[11] (born 6 December 1947) is a British-Canadian cognitive psychologist and computer scientist, most noted for his work on artificial neural networks. Since 2013, he has divided his time working for Google (Google Brain) and the University of Toronto. In 2017, he co-founded and became the Chief Scientific Advisor of the Vector Institute in Toronto.[12][13]\n","With David Rumelhart and Ronald J. Williams, Hinton was co-author of a highly cited paper published in 1986 that popularized the backpropagation algorithm for training multi-layer neural networks,[14] although they were not the first to propose the approach.[15] Hinton is viewed as a leading figure in the deep learning community.[16][17][18][19][20] The dramatic image-recognition milestone of the AlexNet designed in collaboration with his students Alex Krizhevsky[21] and Ilya Sutskever for the ImageNet challenge 2012[22] was a breakthrough in the field of computer vision.[23]\n","Hinton received the 2018 Turing Award, together with Yoshua Bengio and Yann LeCun, for their work on deep learning.[24] They are sometimes referred to as the \"Godfathers of AI\" and \"Godfathers of Deep Learning\",[25][26] and have continued to give public talks together.[27]\n","After his Ph.D. he worked at the University of Sussex, and (after difficulty finding funding in Britain)[29] the University of California, San Diego, and Carnegie Mellon University.[1] He was the founding director of the Gatsby Charitable Foundation Computational Neuroscience Unit at University College London,[1] and is currently[30] a professor in the computer science department at the University of Toronto. He holds a Canada Research Chair in Machine Learning, and is currently an advisor for the Learning in Machines & Brains program at the Canadian Institute for Advanced Research. Hinton taught a free online course on Neural Networks on the education platform Coursera in 2012.[31] Hinton joined Google in March 2013 when his company, DNNresearch Inc., was acquired. He is planning to \"divide his time between his university research and his work at Google\".[32]\n","Hinton's research investigates ways of using neural networks for machine learning, memory, perception and symbol processing. He has authored or co-authored over 200 peer reviewed publications.[2][33]\n","While Hinton was a professor at Carnegie Mellon University (1982–1987), David E. Rumelhart and Hinton and Ronald J. Williams applied the backpropagation algorithm to multi-layer neural networks. Their experiments showed that such networks can learn useful internal representations of data.[14] In an interview of 2018,[34] Hinton said that \"David E. Rumelhart came up with the basic idea of backpropagation, so it's his invention.\" Although this work was important in popularizing backpropagation, it was not the first to suggest the approach.[15] Reverse-mode automatic differentiation, of which backpropagation is a special case, was proposed by Seppo Linnainmaa in 1970, and Paul Werbos proposed to use it to train neural networks in 1974.[15]\n","During the same period, Hinton co-invented Boltzmann machines with David Ackley and Terry Sejnowski.[35] His other contributions to neural network research include distributed representations, time delay neural network, mixtures of experts, Helmholtz machines and Product of Experts. In 2007 Hinton coauthored an unsupervised learning paper titled Unsupervised learning of image transformations.[36] An accessible introduction to Geoffrey Hinton's research can be found in his articles in Scientific American in September 1992 and October 1993.[37]\n","In October and November 2017 respectively, Hinton published two open access research papers[38][39] on the theme of capsule neural networks, which according to Hinton are \"finally something that works well.\"[40]\n","Notable former PhD students and postdoctoral researchers from his group include Peter Dayan,[41] Sam Roweis,[41] Richard Zemel,[3][6] Brendan Frey,[7] Radford M. Neal,[8] Ruslan Salakhutdinov,[9] Ilya Sutskever,[10] Yann LeCun[42] and Zoubin Ghahramani.\n","\"\"\""],"id":"d41ac8d3","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"93e5c8fe"},"source":["## **Define the NLP pipeline**\n","\n","Define the steps necessary to solve the `spacy` task:"],"id":"93e5c8fe"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"91d2c18d","outputId":"64069a4c-1f79-4c71-c51f-9875abc1b4f9"},"source":["# Your code here\n","spacy.cli.download(\"en_core_web_sm\")\n","nlp = spacy.load('en_core_web_sm')"],"id":"91d2c18d","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n"]}]},{"cell_type":"markdown","metadata":{"id":"d496035b"},"source":["## **1. Tokenize the document**\n","\n","Build a list of phrases using `spacy`:"],"id":"d496035b"},{"cell_type":"code","metadata":{"id":"cc563910"},"source":["# Your code here"],"id":"cc563910","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4527c4e2"},"source":["## **2. Preprocess the sentences**\n","\n","Implement the `preprocess` function to clean up the text:\n","\n","* Remove special characters (punctuation marks and numbers)\n","* Convert each word to lowercase.\n","* Remove empty sentences\n","* Remove line breaks, tabs, and repeated spaces."],"id":"4527c4e2"},{"cell_type":"code","metadata":{"id":"872af9e1"},"source":["# Your code here"],"id":"872af9e1","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a3dd5471"},"source":["## **3. Build a TFIDF**\n","\n","Build a TF-IDF representation using `sklearn`:\n","\n","Try different vectorizer settings, including:\n","\n","* With and without idf weighting.\n","* With and without sublinear scaling.\n","* Different normalizations (None, l1, l2)"],"id":"a3dd5471"},{"cell_type":"code","metadata":{"id":"4e8270a2"},"source":["# Your code here"],"id":"4e8270a2","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"512c0df4"},"source":["## **4. Shows the number of sentences and vocabulary size**"],"id":"512c0df4"},{"cell_type":"code","metadata":{"id":"5b809d58"},"source":["# Your code here"],"id":"5b809d58","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"49384865"},"source":["## **5. Display the tfidf representation as a pandas dataframe**"],"id":"49384865"},{"cell_type":"code","metadata":{"id":"c2f8eba9"},"source":["# Your code here"],"id":"c2f8eba9","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"420c9a6c"},"source":["## **6. Estimate the importance of each sentence in the text**\n","\n","Try different aggregation functions (sum, mean, std, var, min, max) to obtain a single number that represents each document:"],"id":"420c9a6c"},{"cell_type":"code","metadata":{"id":"22636cfa"},"source":["# Your code here"],"id":"22636cfa","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"de8f367a"},"source":["## **7. Identify the most important sentences in the text**\n","\n","Find the 10 most important sentences in the text. You must filter them, but keep in mind that they must maintain the order in which they appear in the original text."],"id":"de8f367a"},{"cell_type":"code","source":[],"metadata":{"id":"cCo7LXpRE_F6"},"id":"cCo7LXpRE_F6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c75cbb53"},"source":["# Your code here"],"id":"c75cbb53","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"188ad88d"},"source":["## **8. Try other preprocessing techniques or representation variations to improve results**"],"id":"188ad88d"},{"cell_type":"code","metadata":{"id":"8bc0547b"},"source":["# Your code here"],"id":"8bc0547b","execution_count":null,"outputs":[]}]}