{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e6e5e3f"
      },
      "source": [
        "# Workshop - Summarizing text extractively\n",
        "\n",
        "In this task, we will examine a classic application of TF-IDF for extractive text summarization.\n",
        "\n",
        "## Document retrieval"
      ],
      "id": "6e6e5e3f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebae5a44"
      },
      "source": [
        "\n",
        "## Text summarization\n",
        "\n",
        "Text summarization is a typical natural language processing task that aims to extract relevant information from a given text. There are two main approaches to this task:\n",
        "\n",
        "* **Extractive text summarization**: This task aims to retrieve the most relevant chunks of text that are most likely to summarize the content of the text. In this task, textual chunks, sections, or segments of the text are obtained. For example:\n",
        "> Input: \"Alan Mathison Turing OBE FRS (/ˈtjʊərɪŋ/; June 23, 1912 - June 7, 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher, and theoretical biologist. [6][7] Turing had a major influence on the development of theoretical computer science, providing a formalization of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer.[8][9][10] Turing is widely regarded as the father of theoretical computer science and artificial intelligence.[11]\"\n",
        "\n",
        "  > Output: “Alan Mathison Turing OBE FRS (/ˈtjʊərɪŋ/; June 23, 1912 – June 7, 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher, and theoretical biologist.”\n",
        "* **Abstract text summarization**: This is a task that aims to synthesize the text, i.e., when the summary does not necessarily have to be part of the text. It involves the automatic generation of a coherent and related text."
      ],
      "id": "ebae5a44"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "918c8d00"
      },
      "source": [
        "## Required libraries\n",
        "\n",
        "This task must be resolved with the following dependencies:**"
      ],
      "id": "918c8d00"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a8c3d8c"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "id": "0a8c3d8c",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cde7bf5f"
      },
      "source": [
        "## Data\n",
        "\n",
        "Let's define the text we are going to process:"
      ],
      "id": "cde7bf5f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "d41ac8d3"
      },
      "source": [
        "text = \"\"\"Geoffrey Everest Hinton CC FRS FRSC[11] (born 6 December 1947) is a British-Canadian cognitive psychologist and computer scientist, most noted for his work on artificial neural networks. Since 2013, he has divided his time working for Google (Google Brain) and the University of Toronto. In 2017, he co-founded and became the Chief Scientific Advisor of the Vector Institute in Toronto.[12][13]\n",
        "With David Rumelhart and Ronald J. Williams, Hinton was co-author of a highly cited paper published in 1986 that popularized the backpropagation algorithm for training multi-layer neural networks,[14] although they were not the first to propose the approach.[15] Hinton is viewed as a leading figure in the deep learning community.[16][17][18][19][20] The dramatic image-recognition milestone of the AlexNet designed in collaboration with his students Alex Krizhevsky[21] and Ilya Sutskever for the ImageNet challenge 2012[22] was a breakthrough in the field of computer vision.[23]\n",
        "Hinton received the 2018 Turing Award, together with Yoshua Bengio and Yann LeCun, for their work on deep learning.[24] They are sometimes referred to as the \"Godfathers of AI\" and \"Godfathers of Deep Learning\",[25][26] and have continued to give public talks together.[27]\n",
        "After his Ph.D. he worked at the University of Sussex, and (after difficulty finding funding in Britain)[29] the University of California, San Diego, and Carnegie Mellon University.[1] He was the founding director of the Gatsby Charitable Foundation Computational Neuroscience Unit at University College London,[1] and is currently[30] a professor in the computer science department at the University of Toronto. He holds a Canada Research Chair in Machine Learning, and is currently an advisor for the Learning in Machines & Brains program at the Canadian Institute for Advanced Research. Hinton taught a free online course on Neural Networks on the education platform Coursera in 2012.[31] Hinton joined Google in March 2013 when his company, DNNresearch Inc., was acquired. He is planning to \"divide his time between his university research and his work at Google\".[32]\n",
        "Hinton's research investigates ways of using neural networks for machine learning, memory, perception and symbol processing. He has authored or co-authored over 200 peer reviewed publications.[2][33]\n",
        "While Hinton was a professor at Carnegie Mellon University (1982–1987), David E. Rumelhart and Hinton and Ronald J. Williams applied the backpropagation algorithm to multi-layer neural networks. Their experiments showed that such networks can learn useful internal representations of data.[14] In an interview of 2018,[34] Hinton said that \"David E. Rumelhart came up with the basic idea of backpropagation, so it's his invention.\" Although this work was important in popularizing backpropagation, it was not the first to suggest the approach.[15] Reverse-mode automatic differentiation, of which backpropagation is a special case, was proposed by Seppo Linnainmaa in 1970, and Paul Werbos proposed to use it to train neural networks in 1974.[15]\n",
        "During the same period, Hinton co-invented Boltzmann machines with David Ackley and Terry Sejnowski.[35] His other contributions to neural network research include distributed representations, time delay neural network, mixtures of experts, Helmholtz machines and Product of Experts. In 2007 Hinton coauthored an unsupervised learning paper titled Unsupervised learning of image transformations.[36] An accessible introduction to Geoffrey Hinton's research can be found in his articles in Scientific American in September 1992 and October 1993.[37]\n",
        "In October and November 2017 respectively, Hinton published two open access research papers[38][39] on the theme of capsule neural networks, which according to Hinton are \"finally something that works well.\"[40]\n",
        "Notable former PhD students and postdoctoral researchers from his group include Peter Dayan,[41] Sam Roweis,[41] Richard Zemel,[3][6] Brendan Frey,[7] Radford M. Neal,[8] Ruslan Salakhutdinov,[9] Ilya Sutskever,[10] Yann LeCun[42] and Zoubin Ghahramani.\n",
        "\"\"\""
      ],
      "id": "d41ac8d3",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93e5c8fe"
      },
      "source": [
        "## **Define the NLP pipeline**\n",
        "\n",
        "Define the steps necessary to solve the `spacy` task:"
      ],
      "id": "93e5c8fe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91d2c18d",
        "outputId": "1d6a16de-3c46-4990-8c87-480379412bdb"
      },
      "source": [
        "# Your code here\n",
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "id": "91d2c18d",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d496035b"
      },
      "source": [
        "## **1. Tokenize the document**\n",
        "\n",
        "Build a list of phrases using `spacy`:"
      ],
      "id": "d496035b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc563910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "53895114-438b-4139-8703-79934306d50f"
      },
      "source": [
        "doc = nlp(text) # This line processes the text variable (defined in cell d41ac8d3) using the spaCy language model nlp (loaded in cell 91d2c18d). This creates a Doc object, which is a container for the processed text and provides access to various linguistic annotations.\n",
        "phrases = [sent.text for sent in doc.sents] #  line iterates through the sentences in the doc object (doc.sents) and extracts the text of each sentence (sent.text), creating a list of these sentences called phrases.\n",
        "display(phrases[:2]) #  Display the first 2 phrases as an example"
      ],
      "id": "cc563910",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['Geoffrey Everest Hinton CC FRS FRSC[11] (born 6 December 1947) is a British-Canadian cognitive psychologist and computer scientist, most noted for his work on artificial neural networks.',\n",
              " 'Since 2013, he has divided his time working for Google (Google Brain) and the University of Toronto.']"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4527c4e2"
      },
      "source": [
        "## **2. Preprocess the sentences**\n",
        "\n",
        "Implement the `preprocess` function to clean up the text:\n",
        "\n",
        "* Remove special characters (punctuation marks and numbers)\n",
        "* Convert each word to lowercase.\n",
        "* Remove empty sentences\n",
        "* Remove line breaks, tabs, and repeated spaces."
      ],
      "id": "4527c4e2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "872af9e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "438ca92e-e05d-4644-8f65-f77daece5d70"
      },
      "source": [
        "def preprocess(text):\n",
        "    # Remove special characters (punctuation marks and numbers)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert each word to lowercase.\n",
        "    text = text.lower()\n",
        "    # Remove line breaks, tabs, and repeated spaces.\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Apply the preprocess function to each phrase\n",
        "processed_phrases = [preprocess(phrase) for phrase in phrases]\n",
        "\n",
        "# Remove empty sentences\n",
        "processed_phrases = [phrase for phrase in processed_phrases if phrase]\n",
        "\n",
        "display(processed_phrases[:5]) # Display the first 5 processed phrases as an example"
      ],
      "id": "872af9e1",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['geoffrey everest hinton cc frs frsc born december is a britishcanadian cognitive psychologist and computer scientist most noted for his work on artificial neural networks',\n",
              " 'since he has divided his time working for google google brain and the university of toronto',\n",
              " 'in he cofounded and became the chief scientific advisor of the vector institute in toronto with david rumelhart and ronald j williams hinton was coauthor of a highly cited paper published in that popularized the backpropagation algorithm for training multilayer neural networks although they were not the first to propose the approach hinton is viewed as a leading figure in the deep learning community the dramatic imagerecognition milestone of the alexnet designed in collaboration with his students alex krizhevsky and ilya sutskever for the imagenet challenge was a breakthrough in the field of computer vision hinton received the turing award together with yoshua bengio and yann lecun for their work on deep learning they are sometimes referred to as the godfathers of ai and godfathers of deep learning and have continued to give public talks together after his phd he worked at the university of sussex and after difficulty finding funding in britain the university of california san diego and carnegie mellon university he was the founding director of the gatsby charitable foundation computational neuroscience unit at university college london and is currently a professor in the computer science department at the university of toronto',\n",
              " 'he holds a canada research chair in machine learning and is currently an advisor for the learning in machines brains program at the canadian institute for advanced research',\n",
              " 'hinton taught a free online course on neural networks on the education platform coursera in']"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3dd5471"
      },
      "source": [
        "## **3. Build a TFIDF**\n",
        "\n",
        "Build a TF-IDF representation using `sklearn`:\n",
        "\n",
        "Try different vectorizer settings, including:\n",
        "\n",
        "* With and without idf weighting.\n",
        "* With and without sublinear scaling.\n",
        "* Different normalizations (None, l1, l2)"
      ],
      "id": "a3dd5471"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e8270a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf372c58-1279-4a25-d7bc-13b5625ad610"
      },
      "source": [
        "# Initialize the TF-IDF Vectorizer\n",
        "# We can experiment with different settings here:\n",
        "# - use_idf=True/False: With or without inverse document frequency weighting\n",
        "# - sublinear_tf=True/False: Apply sublinear TF scaling (1 + log(tf))\n",
        "# - norm: Normalization to apply ('l1', 'l2', or None)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    use_idf=True,        # Example: using IDF weighting\n",
        "    sublinear_tf=True,   # Example: applying sublinear TF scaling\n",
        "    norm='l2'            # Example: using L2 normalization\n",
        ")\n",
        "\n",
        "# Fit and transform the processed phrases\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_phrases)\n",
        "\n",
        "# The tfidf_matrix is a sparse matrix. We can convert it to a dense array if needed,\n",
        "# but it's generally more memory efficient to work with the sparse matrix.\n",
        "# tfidf_dense = tfidf_matrix.todense()\n",
        "\n",
        "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)"
      ],
      "id": "4e8270a2",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF matrix shape: (17, 284)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "512c0df4"
      },
      "source": [
        "## **4. Shows the number of sentences and vocabulary size**"
      ],
      "id": "512c0df4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b809d58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdc3007a-39aa-49d3-9b29-a98c45d7b68e"
      },
      "source": [
        "# Get the number of processed sentences\n",
        "num_sentences = len(processed_phrases)\n",
        "# Get the vocabulary size from the TF-IDF vectorizer\n",
        "vocabulary_size = len(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Display the number of sentences and vocabulary size\n",
        "print(f\"Number of sentences: {num_sentences}\")\n",
        "print(f\"Vocabulary size: {vocabulary_size}\")"
      ],
      "id": "5b809d58",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 17\n",
            "Vocabulary size: 284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49384865"
      },
      "source": [
        "## **5. Display the tfidf representation as a pandas dataframe**"
      ],
      "id": "49384865"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2f8eba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "514d609e-de48-4074-b8dc-d1bca3467079"
      },
      "source": [
        "# Get the feature names (words) from the vectorizer\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the sparse TF-IDF matrix to a dense format and then to a pandas DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.todense(), columns=feature_names)\n",
        "\n",
        "# Display the first few rows of the TF-IDF DataFrame\n",
        "display(tfidf_df.head())"
      ],
      "id": "c2f8eba9",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   access  accessible  according  ackley  acquired  advanced   advisor  \\\n",
              "0     0.0         0.0        0.0     0.0       0.0  0.000000  0.000000   \n",
              "1     0.0         0.0        0.0     0.0       0.0  0.000000  0.000000   \n",
              "2     0.0         0.0        0.0     0.0       0.0  0.000000  0.073727   \n",
              "3     0.0         0.0        0.0     0.0       0.0  0.234557  0.204811   \n",
              "4     0.0         0.0        0.0     0.0       0.0  0.000000  0.000000   \n",
              "\n",
              "      after        ai      alex  ...  williams      with      work    worked  \\\n",
              "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.169803  0.000000   \n",
              "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "2  0.142961  0.084435  0.084435  ...  0.073727  0.138781  0.060237  0.084435   \n",
              "3  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "4  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "\n",
              "    working  works      yann    yoshua  zemel  zoubin  \n",
              "0  0.000000    0.0  0.000000  0.000000    0.0     0.0  \n",
              "1  0.312586    0.0  0.000000  0.000000    0.0     0.0  \n",
              "2  0.000000    0.0  0.073727  0.084435    0.0     0.0  \n",
              "3  0.000000    0.0  0.000000  0.000000    0.0     0.0  \n",
              "4  0.000000    0.0  0.000000  0.000000    0.0     0.0  \n",
              "\n",
              "[5 rows x 284 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-feabafb8-3f9c-49c7-89f8-af0bb12777f1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>access</th>\n",
              "      <th>accessible</th>\n",
              "      <th>according</th>\n",
              "      <th>ackley</th>\n",
              "      <th>acquired</th>\n",
              "      <th>advanced</th>\n",
              "      <th>advisor</th>\n",
              "      <th>after</th>\n",
              "      <th>ai</th>\n",
              "      <th>alex</th>\n",
              "      <th>...</th>\n",
              "      <th>williams</th>\n",
              "      <th>with</th>\n",
              "      <th>work</th>\n",
              "      <th>worked</th>\n",
              "      <th>working</th>\n",
              "      <th>works</th>\n",
              "      <th>yann</th>\n",
              "      <th>yoshua</th>\n",
              "      <th>zemel</th>\n",
              "      <th>zoubin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.169803</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.312586</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.073727</td>\n",
              "      <td>0.142961</td>\n",
              "      <td>0.084435</td>\n",
              "      <td>0.084435</td>\n",
              "      <td>...</td>\n",
              "      <td>0.073727</td>\n",
              "      <td>0.138781</td>\n",
              "      <td>0.060237</td>\n",
              "      <td>0.084435</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.073727</td>\n",
              "      <td>0.084435</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.234557</td>\n",
              "      <td>0.204811</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 284 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-feabafb8-3f9c-49c7-89f8-af0bb12777f1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-feabafb8-3f9c-49c7-89f8-af0bb12777f1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-feabafb8-3f9c-49c7-89f8-af0bb12777f1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-41ae3319-3027-47ea-abd5-01b30d58c571\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-41ae3319-3027-47ea-abd5-01b30d58c571')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-41ae3319-3027-47ea-abd5-01b30d58c571 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "420c9a6c"
      },
      "source": [
        "## **6. Estimate the importance of each sentence in the text**\n",
        "\n",
        "Try different aggregation functions (sum, mean, std, var, min, max) to obtain a single number that represents each document:"
      ],
      "id": "420c9a6c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22636cfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0eb8358e-00fa-4c87-9623-4144f94c91b8"
      },
      "source": [
        "# Estimate sentence importance using different aggregation functions\n",
        "\n",
        "# Sum of TF-IDF scores for each sentence\n",
        "sentence_importance_sum = tfidf_df.sum(axis=1)\n",
        "\n",
        "# Mean of TF-IDF scores for each sentence\n",
        "sentence_importance_mean = tfidf_df.mean(axis=1)\n",
        "\n",
        "# Standard deviation of TF-IDF scores for each sentence\n",
        "sentence_importance_std = tfidf_df.std(axis=1)\n",
        "\n",
        "# Variance of TF-IDF scores for each sentence\n",
        "sentence_importance_var = tfidf_df.var(axis=1)\n",
        "\n",
        "# Minimum TF-IDF score for each sentence\n",
        "sentence_importance_min = tfidf_df.min(axis=1)\n",
        "\n",
        "# Maximum TF-IDF score for each sentence\n",
        "sentence_importance_max = tfidf_df.max(axis=1)\n",
        "\n",
        "# Display the importance scores for each aggregation method\n",
        "print(\"Sentence Importance (Sum):\")\n",
        "display(sentence_importance_sum.head())\n",
        "\n",
        "print(\"\\nSentence Importance (Mean):\")\n",
        "display(sentence_importance_mean.head())\n",
        "\n",
        "print(\"\\nSentence Importance (Standard Deviation):\")\n",
        "display(sentence_importance_std.head())\n",
        "\n",
        "print(\"\\nSentence Importance (Variance):\")\n",
        "display(sentence_importance_var.head())\n",
        "\n",
        "print(\"\\nSentence Importance (Minimum):\")\n",
        "display(sentence_importance_min.head())\n",
        "\n",
        "print(\"\\nSentence Importance (Maximum):\")\n",
        "display(sentence_importance_max.head())"
      ],
      "id": "22636cfa",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Importance (Sum):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0     4.755047\n",
              "1     3.702337\n",
              "2    10.497836\n",
              "3     4.602754\n",
              "4     3.447810\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.755047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.702337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.497836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.602754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.447810</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence Importance (Mean):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0    0.016743\n",
              "1    0.013036\n",
              "2    0.036964\n",
              "3    0.016207\n",
              "4    0.012140\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.016743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.013036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.036964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.016207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.012140</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence Importance (Standard Deviation):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0    0.057028\n",
              "1    0.057992\n",
              "2    0.046501\n",
              "3    0.057184\n",
              "4    0.058186\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.057028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.057992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.046501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.057184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.058186</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence Importance (Variance):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0    0.003252\n",
              "1    0.003363\n",
              "2    0.002162\n",
              "3    0.003270\n",
              "4    0.003386\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.003252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.003363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.002162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.003270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.003386</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence Importance (Minimum):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0    0.0\n",
              "1    0.0\n",
              "2    0.0\n",
              "3    0.0\n",
              "4    0.0\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence Importance (Maximum):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0    0.238015\n",
              "1    0.414514\n",
              "2    0.177197\n",
              "3    0.283323\n",
              "4    0.385978\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.238015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.414514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.177197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.283323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.385978</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de8f367a"
      },
      "source": [
        "## **7. Identify the most important sentences in the text**\n",
        "\n",
        "Find the 10 most important sentences in the text. You must filter them, but keep in mind that they must maintain the order in which they appear in the original text."
      ],
      "id": "de8f367a"
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine original phrases with their importance scores (using sum in this example)\n",
        "sentence_scores = pd.DataFrame({'phrase': phrases, 'importance': sentence_importance_sum})\n",
        "\n",
        "# Sort sentences by importance in descending order\n",
        "sorted_sentences = sentence_scores.sort_values(by='importance', ascending=False)\n",
        "\n",
        "# Select the top 10 most important sentences\n",
        "top_10_sentences = sorted_sentences.head(10)\n",
        "\n",
        "# Get the indices of the top 10 sentences in the original order\n",
        "top_10_indices = top_10_sentences.index.tolist()\n",
        "\n",
        "# Sort the indices to maintain the original order\n",
        "top_10_indices.sort()\n",
        "\n",
        "# Get the top 10 sentences in their original order\n",
        "summary_sentences = [phrases[i] for i in top_10_indices]\n",
        "\n",
        "print(\"Top 10 Most Important Sentences (in original order):\")"
      ],
      "metadata": {
        "id": "cCo7LXpRE_F6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a7ab8f8-d77c-48a9-9c57-01b7137716b8"
      },
      "id": "cCo7LXpRE_F6",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Most Important Sentences (in original order):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c75cbb53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c7f22dc-c010-4a87-9b09-463a4dbfae9e"
      },
      "source": [
        "# Display the summary sentences\n",
        "for sentence in summary_sentences:\n",
        "    print(f\"- {sentence}\")"
      ],
      "id": "c75cbb53",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Geoffrey Everest Hinton CC FRS FRSC[11] (born 6 December 1947) is a British-Canadian cognitive psychologist and computer scientist, most noted for his work on artificial neural networks.\n",
            "- In 2017, he co-founded and became the Chief Scientific Advisor of the Vector Institute in Toronto.[12][13]\n",
            "With David Rumelhart and Ronald J. Williams, Hinton was co-author of a highly cited paper published in 1986 that popularized the backpropagation algorithm for training multi-layer neural networks,[14] although they were not the first to propose the approach.[15] Hinton is viewed as a leading figure in the deep learning community.[16][17][18][19][20] The dramatic image-recognition milestone of the AlexNet designed in collaboration with his students Alex Krizhevsky[21] and Ilya Sutskever for the ImageNet challenge 2012[22] was a breakthrough in the field of computer vision.[23]\n",
            "Hinton received the 2018 Turing Award, together with Yoshua Bengio and Yann LeCun, for their work on deep learning.[24] They are sometimes referred to as the \"Godfathers of AI\" and \"Godfathers of Deep Learning\",[25][26] and have continued to give public talks together.[27]\n",
            "After his Ph.D. he worked at the University of Sussex, and (after difficulty finding funding in Britain)[29] the University of California, San Diego, and Carnegie Mellon University.[1] He was the founding director of the Gatsby Charitable Foundation Computational Neuroscience Unit at University College London,[1] and is currently[30] a professor in the computer science department at the University of Toronto.\n",
            "- He holds a Canada Research Chair in Machine Learning, and is currently an advisor for the Learning in Machines & Brains program at the Canadian Institute for Advanced Research.\n",
            "- He is planning to \"divide his time between his university research and his work at Google\".[32]\n",
            "Hinton's research investigates ways of using neural networks for machine learning, memory, perception and symbol processing.\n",
            "- He has authored or co-authored over 200 peer reviewed publications.[2][33]\n",
            "While Hinton was a professor at Carnegie Mellon University (1982–1987), David E. Rumelhart and Hinton and Ronald J. Williams applied the backpropagation algorithm to multi-layer neural networks.\n",
            "- In an interview of 2018,[34] Hinton said that \"David E. Rumelhart came up with the basic idea of backpropagation, so it's his invention.\"\n",
            "- Although this work was important in popularizing backpropagation, it was not the first to suggest the approach.[15] Reverse-mode automatic differentiation, of which backpropagation is a special case, was proposed by Seppo Linnainmaa in 1970, and Paul Werbos proposed to use it to train neural networks in 1974.[15]\n",
            "During the same period, Hinton co-invented Boltzmann machines with David Ackley and Terry Sejnowski.[35]\n",
            "- His other contributions to neural network research include distributed representations, time delay neural network, mixtures of experts, Helmholtz machines and Product of Experts.\n",
            "- An accessible introduction to Geoffrey Hinton's research can be found in his articles in Scientific American in September 1992 and October 1993.[37]\n",
            "\n",
            "- In October and November 2017 respectively, Hinton published two open access research papers[38][39] on the theme of capsule neural networks, which according to Hinton are \"finally something that works well.\"[40]\n",
            "Notable former PhD students and postdoctoral researchers from his group include Peter Dayan,[41]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "188ad88d"
      },
      "source": [
        "## **8. Try other preprocessing techniques or representation variations to improve results**"
      ],
      "id": "188ad88d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bc0547b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "dfcbab45-e927-4309-89b6-542ea8abd7c2"
      },
      "source": [
        "# This cell is for experimenting with different preprocessing techniques or TF-IDF variations\n",
        "\n",
        "# Example 1: Stop word removal and Lemmatization using spaCy\n",
        "# This is typically appliedafter initial tokenization and before building the TF-IDF\n",
        "\n",
        "def preprocess_with_stopwords_lemma(text):\n",
        "    doc = nlp(text) # Process the text with spaCy\n",
        "    # Remove stop words and lemmatize tokens (convert words to their base form)\n",
        "    # Filter out stop words and punctuation\n",
        "    processed_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return \" \".join(processed_tokens)\n",
        "\n",
        "# # Apply the new preprocessing function to the original phrases\n",
        "processed_phrases_v2 = [preprocess_with_stopwords_lemma(phrase) for phrase in phrases]\n",
        "# # Remove any empty strings that might result from preprocessing\n",
        "processed_phrases_v2 = [phrase for phrase in processed_phrases_v2 if phrase]\n",
        "\n",
        "print(\"Processed phrases with stop word removal and lemmatization:\")\n",
        "display(processed_phrases_v2[:5])\n",
        "\n",
        "# Example 2: Experimenting with TF-IDF Vectorizer settings\n",
        "# Re-run the TF-IDF vectorization and subsequent steps with a new vectorizer\n",
        "\n",
        "tfidf_vectorizer_v2 = TfidfVectorizer(\n",
        "    use_idf=False,        # Example: without Inverse Document Frequency weighting\n",
        "    sublinear_tf=False,   # Example: without applying sublinear TF scaling (1 + log(tf))\n",
        "    norm=None,            # Example: without normalization ('l1' or 'l2')\n",
        "    # max_df=0.95,        # Ignore terms that appear in more than 95% of the documents\n",
        "    # min_df=2            # Ignore terms that appear in less than 2 documents\n",
        ")\n",
        "\n",
        "# # Fit and transform either the original processed_phrases or processed_phrases_v2\n",
        "tfidf_matrix_v2 = tfidf_vectorizer_v2.fit_transform(processed_phrases) # Or processed_phrases_v2 if you used the new preprocessing\n",
        "\n",
        "print(\"\\nTF-IDF matrix shape (v2):\", tfidf_matrix_v2.shape)\n",
        "\n",
        "# Remember to re-run the subsequent cells (showing sentence importance, displaying as dataframe, etc.)\n",
        "# after making changes in this cell to see the effect of the variations."
      ],
      "id": "8bc0547b",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed phrases with stop word removal and lemmatization:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['Geoffrey Everest Hinton CC FRS frsc[11 bear 6 December 1947 british canadian cognitive psychologist computer scientist note work artificial neural network',\n",
              " '2013 divide time work Google Google Brain University Toronto',\n",
              " '2017 co found Chief Scientific Advisor Vector Institute Toronto.[12][13 \\n David Rumelhart Ronald J. Williams Hinton co author highly cite paper publish 1986 popularize backpropagation algorithm train multi layer neural networks,[14 propose approach.[15 Hinton view lead figure deep learning community.[16][17][18][19][20 dramatic image recognition milestone AlexNet design collaboration student Alex Krizhevsky[21 Ilya Sutskever ImageNet challenge 2012[22 breakthrough field computer vision.[23 \\n Hinton receive 2018 Turing Award Yoshua Bengio Yann LeCun work deep learning.[24 refer Godfathers AI Godfathers Deep learning\",[25][26 continue public talk together.[27 \\n ph.d. work University Sussex difficulty find funding britain)[29 University California San Diego Carnegie Mellon University.[1 found director Gatsby Charitable Foundation Computational Neuroscience Unit University College London,[1 currently[30 professor computer science department University Toronto',\n",
              " 'hold Canada Research Chair Machine Learning currently advisor Learning Machines Brains program Canadian Institute Advanced Research',\n",
              " 'Hinton teach free online course Neural Networks education platform Coursera 2012.[31']"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF matrix shape (v2): (17, 284)\n"
          ]
        }
      ]
    }
  ]
}