{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic Projections Assignment \ud83d\udcd8\n",
        "\n",
        "This notebook will guide you through an exercise on **semantic projections** using pre-trained word embeddings.  \n",
        "We will:\n",
        "1. Download pre-trained embeddings (GloVe).\n",
        "2. Define a semantic direction (e.g., gender).\n",
        "3. Project words onto this direction.\n",
        "4. Visualize the results in 2D using PCA.\n",
        "5. Reflect on the meaning of these projections.\n",
        "\n",
        "---"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In the commented code where it says **TODO**, you should complete it and make it functional."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup\n",
        "We will start by installing the required libraries and downloading GloVe embeddings."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pretrained GloVe (100d)\n",
        "embeddings_index = api.load(\"glove-wiki-gigaword-100\")\n",
        "print(f\"Loaded {len(embeddings_index.key_to_index)} word vectors.\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Defining a Semantic Direction\n",
        "To explore projections, we need a **semantic direction**.\n",
        "For this example, let's define the **gender direction**:\n",
        "$$ d_{gender} = \\\\vec{he} - \\\\vec{she} $$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the gender direction\n",
        "gender_direction = embeddings_index[\"he\"] - embeddings_index[\"she\"]\n",
        "print(\"Gender direction vector created (shape):\", gender_direction.shape)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to create a semantic direction between any two words\n",
        "def semantic_direction(word1, word2, embeddings):\n",
        "    return embeddings[word1] - embeddings[word2]\n",
        "\n",
        "# Example: royalty direction\n",
        "royalty_direction = semantic_direction(\"king\", \"queen\", embeddings_index)\n",
        "print(\"Royalty direction vector created (shape):\", royalty_direction.shape)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Projecting Words\n",
        "The projection of a word embedding onto the semantic direction is given by:\n",
        "$$ \\\\text{proj}_{d}(w) = \\\\frac{w \\\\cdot d}{\\\\|d\\\\|^2} \\\\, d $$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute projection of a word onto a direction\n",
        "def project_word(word, direction, embeddings):\n",
        "    vec = embeddings[word]\n",
        "    projection = (np.dot(vec, direction) / np.dot(direction, direction)) * direction\n",
        "    return projection\n",
        "\n",
        "# Test with some words\n",
        "words_to_test = [\"man\", \"woman\", \"king\", \"queen\", \"doctor\", \"nurse\"]\n",
        "for w in words_to_test:\n",
        "    proj = project_word(w, gender_direction, embeddings_index)\n",
        "    score = np.dot(embeddings_index[w], gender_direction)\n",
        "    print(f\"{w}: projection score along gender axis = {score:.4f}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Visualization in 2D (PCA)\n",
        "We will project embeddings and their semantic projections into **2D space** using PCA for visualization."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_projections(words, direction, embeddings, title):\n",
        "    vectors = [embeddings[w] for w in words]\n",
        "    projections = [project_word(w, direction, embeddings) for w in words]\n",
        "    \n",
        "    pca = PCA(n_components=2)\n",
        "    all_points = np.vstack([vectors, projections])\n",
        "    reduced = pca.fit_transform(all_points)\n",
        "    \n",
        "    n = len(words)\n",
        "    orig_points = reduced[:n]\n",
        "    proj_points = reduced[n:]\n",
        "    \n",
        "    plt.figure(figsize=(8,6))\n",
        "    for i, word in enumerate(words):\n",
        "        plt.scatter(orig_points[i,0], orig_points[i,1], color='blue')\n",
        "        plt.text(orig_points[i,0]+0.02, orig_points[i,1], word, fontsize=10)\n",
        "        plt.scatter(proj_points[i,0], proj_points[i,1], color='red', marker='x')\n",
        "        plt.plot([orig_points[i,0], proj_points[i,0]], [orig_points[i,1], proj_points[i,1]], 'k--', alpha=0.5)\n",
        "    \n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"PCA 1\")\n",
        "    plt.ylabel(\"PCA 2\")\n",
        "    plt.show()\n",
        "\n",
        "# Example visualization: gender axis\n",
        "visualize_projections(words_to_test, gender_direction, embeddings_index, \"Word Embeddings and Projections onto Gender Direction\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Reflection\n",
        "- What do you observe about the position of words like *king/queen* and *man/woman*?\n",
        "- Do professions like *doctor* or *nurse* show bias when projected?\n",
        "- How could you use this technique to **debias embeddings**?\n",
        "---\n",
        "## \u2705 Final Task: Extend this notebook by trying a different semantic direction, e.g.,\n",
        "$$ d_{royalty} = \\\\vec{king} - \\\\vec{queen} $$, and analyze the projections."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Example with royalty direction\n",
        "royalty_words = [\"king\", \"queen\", \"prince\", \"princess\", \"man\", \"woman\"]\n",
        "visualize_projections(royalty_words, royalty_direction, embeddings_index, \"Word Embeddings and Projections onto Royalty Direction\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}