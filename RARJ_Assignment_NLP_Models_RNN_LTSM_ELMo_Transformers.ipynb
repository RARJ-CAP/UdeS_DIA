{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Overview\n",
        "This assignment focuses on understanding and applying four key approaches to processing sequential text data:\n",
        "\n",
        "Recurrent Neural Networks (**RNN**s) - Basic sequential processing\n",
        "\n",
        "Long Short-Term Memory (**LSTM**) - Advanced sequential processing with memory\n",
        "\n",
        "**ELMo** - Context-aware embeddings using bidirectional LSTMs\n",
        "\n",
        "**Transformers** - Attention-based parallel processing"
      ],
      "metadata": {
        "id": "c9iTUcVNSMBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base code"
      ],
      "metadata": {
        "id": "0W9xG20JiIog"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RYrN9GwO6vm"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"ðŸ“š Assignment Environment Setup Complete!\")\n",
        "print(\"ðŸŽ¯ Ready to tackle RNN, LSTM, ELMo, and Transformers!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecture Comparison"
      ],
      "metadata": {
        "id": "LjsdeGslSM0f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "888daef0"
      },
      "source": [
        "| Aspect | RNN | LSTM | ELMo | Transformers |\n",
        "| :--- | :---: | :---: | :---: | :---: |\n",
        "| Processing Type | Sequential | Sequential | Sequential | Parallel |\n",
        "| Memory Mechanism | Hidden state | Gated memory cells | Contextualized embeddings via BiLSTM | Self-attention mechanism |\n",
        "| Parallel Training | No | No | No | Yes |\n",
        "\n",
        "# Explanation\n",
        "\n",
        "**Processing Type**:\n",
        "RNN, LSTM, and ELMo process input sequentially, which limits parallelization. Transformers process input in parallel using self-attention.\n",
        "\n",
        "**Memory Mechanism**:\n",
        "RNN uses a hidden state to carry information. LSTM improves this with gated cells to manage long-term dependencies. ELMo uses BiLSTM layers to generate contextual embeddings. Transformers use self-attention to capture dependencies across the entire sequence.\n",
        "\n",
        "**Parallel Training**:\n",
        "RNN, LSTM, and ELMo are inherently sequential, making parallel training difficult. Transformers allow parallelization, which significantly speeds up training."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Analysis\n",
        "For each scenario below, choose the most appropriate model (RNN, LSTM, ELMo, or Transformers) and explain."
      ],
      "metadata": {
        "id": "FXdlEffasUXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Scenario A**  \n",
        "**Task:** Building a simple next-word prediction system for short sentences (5â€“10 words) with limited computational resources.  \n",
        "**My Choice:** **RNN**  \n",
        "**Explanation:** RNNs are lightweight and suitable for short sequences where complex memory mechanisms arenâ€™t necessary. They require fewer resources and can perform adequately for simple tasks like next-word prediction in short texts.\n",
        "\n",
        "---\n",
        "\n",
        "### **Scenario B**  \n",
        "**Task:** Analyzing sentiment in movie reviews where the sentiment often depends on words that appear far apart in the text.  \n",
        "**My Choice:** **LSTM**  \n",
        "**Explanation:** LSTMs are designed to capture long-range dependencies in text, making them ideal for sentiment analysis where key phrases may be separated by many words. Their gated memory cells help retain relevant information over longer sequences.\n",
        "\n",
        "---\n",
        "\n",
        "### **Scenario C**  \n",
        "**Task:** Creating context-aware word embeddings for a domain-specific corpus (medical texts) where the same word has different meanings.  \n",
        "**My Choice:** **ELMo**  \n",
        "**Explanation:** ELMo generates dynamic, context-sensitive embeddings using BiLSTMs, which is crucial in domains like medicine where word meaning heavily depends on context. It captures semantic nuances better than static embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "### **Scenario D**  \n",
        "**Task:** Building a state-of-the-art question-answering system that needs to understand complex relationships between all words in long documents.  \n",
        "**My Choice:** **Transformers**  \n",
        "**Explanation:** Transformers use self-attention to model relationships between all words in a sequence, regardless of distance. This makes them ideal for tasks requiring deep understanding of context and long-range dependencies, like question answering.\n",
        "\n",
        "---\n",
        "\n",
        "### **Scenario E**  \n",
        "**Task:** Processing time series data (stock prices) where you need to remember patterns from many time steps ago.  \n",
        "**My Choice:** **LSTM**  \n",
        "**Explanation:** LSTMs are well-suited for time series tasks due to their ability to retain information over long sequences. Their memory cells help capture temporal patterns and trends that are crucial in financial forecasting."
      ],
      "metadata": {
        "id": "KnKZ4cTZRACr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Implementation\n",
        "*RNN*s"
      ],
      "metadata": {
        "id": "6tJ_p73_tAEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "def generate_sequences(n_samples=200, seq_len=5):\n",
        "    X, y = [], []\n",
        "    for _ in range(n_samples-15):\n",
        "        seq = np.random.randint(0, 10, size=seq_len)\n",
        "        label = int(np.all(np.diff(seq) > 0))  # 1 if strictly increasing\n",
        "        X.append(seq)\n",
        "        y.append(label)\n",
        "\n",
        "    for i in range(15): # Introduce examples that are increasing\n",
        "      seq = np.array([i for i in range(5)])\n",
        "      label = 1\n",
        "      X.append(seq)\n",
        "      y.append(label)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X, y = generate_sequences()\n",
        "X_tensor = torch.tensor(X, dtype=torch.long)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "class SimpleRNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=10, hidden_size=16):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)  # [batch_size, seq_len, hidden_size]\n",
        "        _, h_n = self.rnn(x)  # h_n: [1, batch_size, hidden_size]\n",
        "        out = self.fc(h_n.squeeze(0))  # [batch_size, 2]\n",
        "        return out\n",
        "\n",
        "# Training setup\n",
        "model = SimpleRNNClassifier()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "n_epochs = 20\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_tensor)\n",
        "    loss = criterion(outputs, y_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "UEvP558LRoeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*LSTM*"
      ],
      "metadata": {
        "id": "QvksL9R0SKPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Dataset: small text\n",
        "text = \"hello world\"\n",
        "chars = list(set(text))\n",
        "char2idx = {c: i for i, c in enumerate(chars)}\n",
        "idx2char = {i: c for c, i in char2idx.items()}\n",
        "\n",
        "seq = [char2idx[c] for c in text]\n",
        "\n",
        "X = torch.tensor(seq[:-1]).unsqueeze(0)  # input shape: [1, seq_len]\n",
        "y = torch.tensor(seq[1:]).unsqueeze(0)   # target shape: [1, seq_len]\n",
        "\n",
        "# Model definition\n",
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, h=None):\n",
        "        x = self.embed(x)  # [batch_size, seq_len, hidden_size]\n",
        "        out, h = self.lstm(x, h)  # out: [batch_size, seq_len, hidden_size]\n",
        "        out = self.fc(out)  # [batch_size, seq_len, vocab_size]\n",
        "        return out, h\n",
        "\n",
        "# Training setup\n",
        "model = CharLSTM(len(chars), hidden_size=16)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "n_epochs = 50\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output, _ = model(X)\n",
        "    loss = criterion(output.view(-1, len(chars)), y.view(-1))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "pBR4KPggSKm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*ELMo*"
      ],
      "metadata": {
        "id": "GarFMfTkS9Dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load ELMo model\n",
        "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")\n",
        "\n",
        "# Sentences\n",
        "sentences = [\n",
        "    \"He draw a portrait of his wife\",\n",
        "    \"The football match ended in a draw\"\n",
        "]\n",
        "\n",
        "# Compute ELMo embeddings\n",
        "embeddings = elmo.signatures[\"default\"](tf.constant(sentences))[\"elmo\"]  # shape: [batch_size, seq_len, 1024]\n",
        "\n",
        "# Extract embeddings for the word 'bank'\n",
        "target_word = \"draw\"\n",
        "for i, sentence in enumerate(sentences):\n",
        "    words = sentence.split()\n",
        "    try:\n",
        "        index = words.index(target_word)\n",
        "        word_embedding = embeddings[i][index].numpy()\n",
        "        print(f\"Embedding for '{target_word}' in sentence {i+1}:\")\n",
        "        print(word_embedding[:10])  # print first 10 values for brevity\n",
        "    except ValueError:\n",
        "        print(f\"'{target_word}' not found in sentence {i+1}\")\n"
      ],
      "metadata": {
        "id": "wITffxCeS9fQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What happened if we are trying to found the embedings of a word that doesn't appear in the input sentences?**"
      ],
      "metadata": {
        "id": "lBWlx1sugyd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentences\n",
        "sentences = [\n",
        "    \"He draw a portrait of his wife\",\n",
        "    \"The football match ended in a draw\"\n",
        "]\n",
        "\n",
        "# Compute ELMo embeddings\n",
        "embeddings = elmo.signatures[\"default\"](tf.constant(sentences))[\"elmo\"]  # shape: [batch_size, seq_len, 1024]\n",
        "\n",
        "# Extract embeddings for the word 'bank'\n",
        "target_word = \"bank\"\n",
        "for i, sentence in enumerate(sentences):\n",
        "    words = sentence.split()\n",
        "    try:\n",
        "        index = words.index(target_word)\n",
        "        word_embedding = embeddings[i][index].numpy()\n",
        "        print(f\"Embedding for '{target_word}' in sentence {i+1}:\")\n",
        "        print(word_embedding[:10])  # print first 10 values for brevity\n",
        "    except ValueError:\n",
        "        print(f\"'{target_word}' not found in sentence {i+1}\")"
      ],
      "metadata": {
        "id": "TLHwkvUKgywT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Transformer*\n"
      ],
      "metadata": {
        "id": "yv0zAtEhhYBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Input sentence with a masked token\n",
        "sentence = \"The cat sat on the [MASK].\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "\n",
        "# Get model predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predictions = outputs.logits\n",
        "\n",
        "# Find the index of the [MASK] token\n",
        "mask_token_index = torch.where(inputs.input_ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "# Get the top predicted token at the mask position\n",
        "top_token_id = predictions[0, mask_token_index].argmax(dim=-1)\n",
        "predicted_token = tokenizer.decode(top_token_id)\n",
        "\n",
        "print(f\"Predicted word: {predicted_token}\")"
      ],
      "metadata": {
        "id": "9SVVWa0vhZhI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}