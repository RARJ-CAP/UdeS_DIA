{"nbformat":4,"nbformat_minor":5,"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8149f726"},"source":["# Classic text representation techniques\n","\n","This notebook presents the most typical representation techniques that have been used in natural language processing. These representations are motivated by probability and have traditionally been used as a baseline for text representation. This representation allows us to address common tasks in NLP such as information retrieval, text generation, autocompletion, text classification, topic modeling, among others.\n"],"id":"8149f726"},{"cell_type":"markdown","metadata":{"id":"f949356e"},"source":["## Bag of Words\n","\n","The bag-of-words (BoW) is the simplest representation to use. It can be easily calculated for different texts and allows computational calculations to be parallelized, which is useful for large text datasets.\n","\n","# Classic text representation techniques\n","\n","<img src=\"https://miro.medium.com/max/1134/1*lhH8dFbK5_saNe4kcWXwiA.png\" width=500>\n","\n","\n","The formal definition of BoW assumes that there is a finite set of words or vocabulary $\\mathcal{V} = \\{w_1, w_2, \\dots, w_m\\}$ and that we have a corpus of $N$ documents $\\mathbf{d}_i=[w_a, w_b, \\dots, w_d]$ composed of ordered words from the vocabulary $\\mathcal{V}$.\n","\n","Therefore, the BoW representation can be calculated as the categorical distribution of words in a single document $P(w_j|d_i)$, as shown in the following equation:\n","\n","$$\n","P(w_j|d_i) \\propto \\#(w_j \\in d_i)\\\\\n","P(w_j|d_i) = \\frac{\\#(w_j \\in d_i)}{|d_i|}\n","$$\n","\n","This corresponds to the normalized number of occurrences of each word in the document.\n","\n","For practical examples, we will import some necessary libraries:\n"],"id":"f949356e"},{"cell_type":"code","metadata":{"lines_to_next_cell":1,"id":"498ca0ec"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from collections import Counter\n","from wordcloud import WordCloud, STOPWORDS\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud"],"id":"498ca0ec","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7d5cac12"},"source":["In addition, we will define some help functions:"],"id":"7d5cac12"},{"cell_type":"code","metadata":{"lines_to_next_cell":1,"id":"bd33b320"},"source":["def show_bow(X, vocab):\n","    row_names = [f\"d_{i}\" for i in range(X.shape[0])]\n","    return pd.DataFrame(\n","            data=X,\n","            columns=vocab,\n","            index=row_names\n","            )"],"id":"bd33b320","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5e4d7479"},"source":["### BoW from Scratch\n","\n","We will implement BoW using Python and Numpy. For this example, we will use some sample texts:"],"id":"5e4d7479"},{"cell_type":"code","metadata":{"id":"852d5cf4"},"source":["data = [\n","        \"the black puma chased a rabbit\", # each string is a document\n","        \"the rabbit is an animal that is usually fast\",\n","        \"the lion is a big cat but a house cat is really different from a lion\"\n","        ] # the complete list is the corpus"],"id":"852d5cf4","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"42d64470"},"source":["First, let's calculate the vocabulary:"],"id":"42d64470"},{"cell_type":"code","metadata":{"id":"bdfca7d2"},"source":["all_text = \" \".join(data)\n","tokens = all_text.split()\n","vocab = np.unique(tokens)\n","print(vocab)\n","print(f\"The vocabulary has the following size {vocab.size}\")"],"id":"bdfca7d2","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cd90e94d"},"source":["We will also need a dictionary with the words and tokens:"],"id":"cd90e94d"},{"cell_type":"code","metadata":{"id":"78dd031d"},"source":["indexes = {word: idx for word, idx in zip(vocab, np.arange(vocab.size))}"],"id":"78dd031d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3c844fdc"},"source":["This means that the BoW will have the following form:"],"id":"3c844fdc"},{"cell_type":"code","metadata":{"id":"2c61f1d6"},"source":["shape = (len(data), vocab.size)\n","print(shape)"],"id":"2c61f1d6","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9dad1f3a"},"source":["Let's create a numpy array that will contain the word counts for each document:"],"id":"9dad1f3a"},{"cell_type":"code","metadata":{"lines_to_next_cell":1,"id":"217cf618"},"source":["X = np.zeros(shape, dtype=np.float64)"],"id":"217cf618","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"76b1964e"},"source":["For the calculation, we will define a function that calculates the word counts for a single document."],"id":"76b1964e"},{"cell_type":"code","metadata":{"lines_to_next_cell":1,"id":"dd3e2246"},"source":["def counts_document(d_i, vocab_size=100):\n","    tokens = d_i.split()\n","    words, counts = np.unique(\n","            tokens,\n","            return_counts=True\n","            )\n","    index = [indexes[word] for word in words]\n","    row = np.zeros((vocab_size, ))\n","    row[index] = counts\n","    return row"],"id":"dd3e2246","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"274a3910"},"source":["Now, let's take a look at the complete BoW:"],"id":"274a3910"},{"cell_type":"code","metadata":{"id":"99953057"},"source":["for i in range(len(data)):\n","    X[i] = counts_document(\n","            data[i],\n","            vocab_size=vocab.size\n","            )\n","\n","show_bow(X, vocab)"],"id":"99953057","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6682ec41"},"source":["This result corresponds to the unnormalized BoW. As we will see later, there are different normalization strategies that can be applied to the BoW. For now, in order to have valid probability distributions, we will convert the absolute frequencies into relative frequencies:"],"id":"6682ec41"},{"cell_type":"code","metadata":{"id":"93fbaf2d"},"source":["sums = X.sum(axis=0).reshape(1, -1)\n","BoW = X / sums\n","show_bow(BoW, vocab)"],"id":"93fbaf2d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"72e0e332"},"source":["The above implementation is useful for academic purposes, however, there are Python packages that already do this with high performance and various configurations. In this case, we will explore the `CountVectorizer` from the `sklearn` library.\n","\n","Let's see how to calculate BoW with this component:"],"id":"72e0e332"},{"cell_type":"code","metadata":{"id":"52e81525"},"source":["vect = CountVectorizer().fit(data)\n","BoW = vect.transform(data).toarray()\n","show_bow(BoW, vect.get_feature_names_out())"],"id":"52e81525","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"192b82b9"},"source":["The `CountVectorizer` allows for several filters. We will present some examples using the dataset we will use in this notebook:"],"id":"192b82b9"},{"cell_type":"markdown","metadata":{"id":"74bb137a"},"source":["This data frame contains tweets related to airlines and their sentiment analysis scores. You can find it at: https://www.kaggle.com/crowdflower/twitter-airline-sentiment.\n","\n","We can check different properties in this data frame:"],"id":"74bb137a"},{"cell_type":"code","source":["from google.colab import files\n","\n","uploaded = files.upload()\n"],"metadata":{"id":"k2lGDvcB-zAl"},"id":"k2lGDvcB-zAl","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import io\n","\n","data = pd.read_csv(io.StringIO(uploaded['Tweets.csv'].decode('utf-8')))"],"metadata":{"id":"G6RHRx6E-6lw"},"id":"G6RHRx6E-6lw","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c8ce8a7a"},"source":["data.info()"],"id":"c8ce8a7a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5291feb7"},"source":["data.describe(include='O')"],"id":"5291feb7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f262818d"},"source":["data.head()"],"id":"f262818d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3201f407"},"source":["In this case, we will focus on the `text` and `airline_sentiment` columns:"],"id":"3201f407"},{"cell_type":"code","metadata":{"id":"f908331a"},"source":["df = data[[\"text\", \"airline_sentiment\"]].copy()\n","df"],"id":"f908331a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3cc6a05d"},"source":["Let's build the BoW representation for this data."],"id":"3cc6a05d"},{"cell_type":"code","metadata":{"id":"e7ea4087"},"source":["vect = CountVectorizer().fit(\n","        df[\"text\"]\n","        )\n","vect"],"id":"e7ea4087","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a15f72e0"},"source":["Now, the word counts for each tweet would be:"],"id":"a15f72e0"},{"cell_type":"code","metadata":{"id":"b5fdd34c"},"source":["X = vect.transform(df[\"text\"])\n","show_bow(\n","        X.toarray(),\n","        vect.get_feature_names_out()\n","        )"],"id":"b5fdd34c","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"98a9d8d0"},"source":["As you can see, there are some rare terms that appear once or twice. In addition, there are terms that may appear in every document. We are going to filter these terms, but **why would we want to filter them?**"],"id":"98a9d8d0"},{"cell_type":"code","metadata":{"lines_to_next_cell":0,"id":"9e5f45fe"},"source":["vect = CountVectorizer(\n","        min_df=0.025, # minimum relative frequency to consider a term\n","        max_df=0.9 # maximum relative frequency to consider a term\n","        ).fit(\n","                df[\"text\"]\n","                )"],"id":"9e5f45fe","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"47d6161a"},"source":["X = vect.transform(df[\"text\"])\n","show_bow(\n","        X.toarray(),\n","        vect.get_feature_names_out()\n","        )"],"id":"47d6161a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1b465cab"},"source":["We can also specify a list of stop words that we want to exclude from the BoW:"],"id":"1b465cab"},{"cell_type":"code","metadata":{"lines_to_next_cell":0,"id":"cc680be4"},"source":["vect = CountVectorizer(\n","        min_df=0.025, # minimum relative frequency to consider a term\n","        max_df=0.9, # maximum relative frequency to consider a term\n","        stop_words=[\"and\",\"about\", \"after\", \"again\"]\n","        ).fit(\n","                df[\"text\"]\n","                )"],"id":"cc680be4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e91b008c"},"source":["X = vect.transform(df[\"text\"])\n","show_bow(\n","        X.toarray(),\n","        vect.get_feature_names_out()\n","        )"],"id":"e91b008c","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ewHSBSu5Mcf-"},"source":["doc = data.text.str.cat(sep=' ')"],"id":"ewHSBSu5Mcf-","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y6edg3j-NhGs"},"source":["wordcloud = WordCloud(width = 800, height = 800,\n","                background_color ='white',\n","                min_font_size = 10).generate(doc)"],"id":"Y6edg3j-NhGs","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mjk5wMbOM4me"},"source":["plt.figure(figsize = (8, 8), facecolor = None)\n","plt.imshow(wordcloud)\n","plt.axis(\"off\")\n","plt.tight_layout(pad = 0)\n","\n","plt.show()"],"id":"Mjk5wMbOM4me","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vTGr_7ex1uIA"},"source":["### Quiz 1:\n","\n","Include a word cloud here with the 100 most frequently used terms."],"id":"vTGr_7ex1uIA"},{"cell_type":"code","metadata":{"id":"C4-bAuJS1tkb"},"source":["# Insert your code"],"id":"C4-bAuJS1tkb","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b210c67c"},"source":["## TF-IDF\n","\n","There are some words that are too common in the corpus and are not strictly stopwords. These words differ between datasets, and we must find appropriate ways to identify them.\n","\n","One of the most common approaches is term frequency inverse document frequency (TF-IDF). This is a weighting scheme that aims to weight words by their appearance in different documents. TF-IDF extends BoW by multiplying the frequency of each term $t_j$ by a weight $w_j$ as follows\n","\n","$$\n","\\text(TFIDF)(t_i|d_j)=\\text{TF}(t_i | d_j) w_i\n","$$\n","\n","In this way, we assign a lower weight to terms that are common in different documents and a higher weight to rare terms. There are several ways to calculate $w_i$, the most common approach being the *inverse document frequency* $w_{idf}(t_i)$, which is calculated as follows:\n","\n","$$\n","w_{idf}(t_i)=1+\\log{\\frac{N}{1+df(t_i)}}\n","$$\n","\n","Where $N$ is the number of documents in the corpus and $df(t_i)$ is the number of documents containing the term $t_i$.\n","\n","Now, we can calculate a TF-IDF representation using the `TfidfVectorizer`:"],"id":"b210c67c"},{"cell_type":"code","metadata":{"id":"07ceb236"},"source":["vect = TfidfVectorizer().fit(df[\"text\"])"],"id":"07ceb236","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6d9636d6"},"source":["X = vect.transform(df[\"text\"])\n","show_bow(\n","        X.toarray(),\n","        vect.get_feature_names_out()\n","        )"],"id":"6d9636d6","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bdd3f6be"},"source":["The `TfidfVectorizer` is similar to the `CountVectorizer`, so we can use the same parameters to filter words by frequency:"],"id":"bdd3f6be"},{"cell_type":"code","metadata":{"lines_to_next_cell":0,"id":"9bfe4285"},"source":["vect = TfidfVectorizer(\n","        min_df=0.025,\n","        max_df=0.9\n","        ).fit(df[\"text\"])"],"id":"9bfe4285","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"lines_to_next_cell":2,"id":"8d163e52"},"source":["X = vect.transform(df[\"text\"])\n","show_bow(\n","        X.toarray(),\n","        vect.get_feature_names_out()\n","        )"],"id":"8d163e52","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6b08bcd5"},"source":["In addition, we can change the scale of the frequencies using a logarithm (sublinear scaling)."],"id":"6b08bcd5"},{"cell_type":"code","metadata":{"id":"bf580d3d"},"source":["vect = TfidfVectorizer(\n","        min_df=0.1,\n","        max_df=0.9,\n","        sublinear_tf=True\n","        ).fit(df[\"text\"])"],"id":"bf580d3d","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5cbcbe9d"},"source":["X = vect.transform(df[\"text\"])\n","show_bow(\n","        X.toarray(),\n","        vect.get_feature_names_out()\n","        )"],"id":"5cbcbe9d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5fUTC13j34rf"},"source":["### Quiz 2:\n","\n","Include a word cloud here with the 100 most frequently used terms."],"id":"5fUTC13j34rf"},{"cell_type":"code","metadata":{"id":"tNSAMx6dPJa2"},"source":["#your code here\n","data_frame_tfidf = show_bow(X.toarray(),\n","                          vect.get_feature_names_out())"],"id":"tNSAMx6dPJa2","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DG6lwfV6PrUv"},"source":["# data_frame_tfidf[:100]"],"id":"DG6lwfV6PrUv","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tQ4P9QkSPKRl"},"source":["\n","# top_100_freq ="],"id":"tQ4P9QkSPKRl","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TxdfVrF936aP"},"source":["#top_100_freq.to_csv('top_100_freq.csv')\n","#insert your code"],"id":"TxdfVrF936aP","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cf6c4665"},"source":["## Bag of N-Gramas\n","\n","Bag-of-N-Grams (BoN) is a representation that extends the BoW representation. It counts sequences of tokens (usually characters or words) of different lengths. For example, we can construct 3-grams for the following text:\n","\n","<emph>the little apple</emph>\n","\n","For this sequence, the possible 3-grams are:\n","\n","``python\n","“the”, “th ”, “e l”, “ li”, “lit”, “itt”, “ttl”, “tle”, “le ”, “e a”, “ap”, “app”, ‘ppl’, “ple”\n","```\n","\n","We will implement this natively in Python. First, we define the input text."],"id":"cf6c4665"},{"cell_type":"code","metadata":{"id":"f6153479"},"source":["text = \"the little apple\"\n","print(text)"],"id":"f6153479","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"430b0855"},"source":["\n","Now, we must divide the text into sequences of a certain length:"],"id":"430b0855"},{"cell_type":"code","metadata":{"id":"77dfd322"},"source":["n = 3\n","grams = [\n","        text[i: i + n]\n","        for i in range(len(text) - 2)\n","        ]\n","print(grams)"],"id":"77dfd322","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1594f202"},"source":["\n","Therefore, the representation of N-grams is calculated by counting the number of occurrences of each unique sequence of characters. Let's look at an example with a text containing repeated 3-grams:"],"id":"1594f202"},{"cell_type":"code","metadata":{"id":"7481dca0"},"source":["text = \"the man who sold the world to another man\"\n","print(text)"],"id":"7481dca0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d1522e01"},"source":["n = 3\n","grams = [\n","        text[i: i + n]\n","        for i in range(len(text) - 2)\n","        ]\n","print(grams)"],"id":"d1522e01","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"91a70bd9"},"source":["n_grams = Counter(grams)\n","print(n_grams)"],"id":"91a70bd9","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8c9afe6b"},"source":["In Python, we have other implementations that are much more efficient and can calculate N-grams for real-world applications. Let's look at the example with the airline sentiment dataset:"],"id":"8c9afe6b"},{"cell_type":"code","metadata":{"id":"3554ac78"},"source":["vect = CountVectorizer(\n","        analyzer=\"char\", # this specifies that We want to work at characters level.\n","        ngram_range=(3, 3) # this specifies the range of sequences to consider, in this case only 3.\n","        ).fit(df[\"text\"])"],"id":"3554ac78","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e632869a"},"source":["Now, we can determine the representation of the tri-grams:"],"id":"e632869a"},{"cell_type":"code","metadata":{"id":"b089c913"},"source":["X = vect.transform(df[\"text\"])\n","show_bow(X.toarray(), vect.get_feature_names_out())"],"id":"b089c913","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"11f12bc5"},"source":["Note that there is a wide range of sequences of size 3. We can filter some of them in a similar way to the BoW representation, i.e., by filtering out terms that are too common or too rare."],"id":"11f12bc5"},{"cell_type":"code","metadata":{"id":"1167e1c7"},"source":["vect = CountVectorizer(\n","        analyzer=\"char\", # this specifies that We want to work at characters level.\n","        ngram_range=(3, 3), # this specifies the range of sequences to consider, in this case only 3.\n","        min_df=0.025,\n","        max_df=0.9\n","        ).fit(df[\"text\"])"],"id":"1167e1c7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8826c7f"},"source":["X = vect.transform(df[\"text\"])\n","show_bow(X.toarray(), vect.get_feature_names_out())"],"id":"e8826c7f","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9f1b3e28"},"source":["Similarly, we can construct a BoN using sequences of words. For example, let's construct a BoN for sequences of 1, 2, and 3 words:"],"id":"9f1b3e28"},{"cell_type":"code","metadata":{"id":"0b28ccef"},"source":["vect = CountVectorizer(\n","        analyzer=\"word\", # this specifies that We want to work at characters level.\n","        ngram_range=(1, 3), # this specifies the range of sequences to consider, in this case only 3.\n","        min_df=0.010,\n","        max_df=0.9\n","        ).fit(df[\"text\"])"],"id":"0b28ccef","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"04636056"},"source":["X = vect.transform(df[\"text\"])\n","show_bow(X.toarray(), vect.get_feature_names_out())"],"id":"04636056","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2efea5bd"},"source":["## Autosuggestions with BoN\n","\n","This is a typical example based on the probabilistic interpretation of BoN as Markov chains.\n","\n","The general idea is that we are going to estimate the conditional probability:\n","\n","$$\n","P(t_k|t_{k-1}, t_{k-2}, \\dots, t_0)\n","$$\n","\n","However, using a Markov assumption, we intuitively restrict this conditional probability:\n","\n","* When we have a first-order Markov chain, the probability $P(t_k|t_{k-1})$ of a single token $t_k$ occurring depends only on the previous token $t_{k-1}$.\n","* When we have a second-order Markov chain, the probability $P(t_k|t_{k-1}, t_{k-2})$ of occurrence of a single token $t_k$ depends on the last two tokens $t_{k-1}$.\n","\n","The BoN is related to the previous conditional probability; in fact, a BoN corresponds to the estimation of the joint probability $P(t_k, t_{k-1}, t_{k-2}, \\dots)$, which can be used to calculate the conditional probability of a sequence of size $N - 1$:\n","\n","$$\n","P(t_k| t_{k-1}, t_{k-2}, \\dots) = P(t_k, t_{k-1}, t_{k-2}, \\dots)\n","$$\n","\n","Let's look at this implementation for auto-suggesting the next character for a specific word:"],"id":"2efea5bd"},{"cell_type":"code","metadata":{"id":"49a2f788"},"source":["text = \"Once upon a time, there was a child that tried to climb the highest mountain in the world. \\\"Mark my words, I'll achieve it.\\\", he said.\"\n","\n","vect = CountVectorizer(\n","        analyzer=\"char\",\n","        ngram_range=(3, 3)\n","        ).fit([text])"],"id":"49a2f788","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b1454892"},"source":["X = vect.transform([text])\n","df2 = show_bow(X.toarray(), vect.get_feature_names_out())\n","df2"],"id":"b1454892","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5dad88dd"},"source":["Now, we can divide the sequences"],"id":"5dad88dd"},{"cell_type":"code","metadata":{"id":"ddd9bcb7"},"source":["counts = df2.to_dict()\n","counts = {key: val[\"d_0\"] for key, val in counts.items()}\n","counts"],"id":"ddd9bcb7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f6ed7a78"},"source":["Let's calculate the probabilities:"],"id":"f6ed7a78"},{"cell_type":"code","metadata":{"lines_to_next_cell":1,"id":"da3d9c12"},"source":["probs = {}\n","for token, count in counts.items():\n","    if token[:2] not in probs:\n","        probs[token[:2]] = [(token[-1], count)]\n","    else:\n","        probs[token[:2]].append((token[-1], count))\n","\n","print(probs)"],"id":"da3d9c12","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ed93d149"},"source":["Let's look at some examples:"],"id":"ed93d149"},{"cell_type":"code","metadata":{"lines_to_next_cell":1,"id":"e5b468ff"},"source":["def complete_word(word):\n","    seq = word[-2:]\n","    seq_probs = sorted(\n","            probs[seq],\n","            key=lambda x: x[1],\n","            reverse=True\n","            )\n","    print(seq_probs[0][0])"],"id":"e5b468ff","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4db6179d"},"source":["complete_word(\"Onc\")"],"id":"4db6179d","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6781cde3"},"source":["complete_word(\"Upo\")"],"id":"6781cde3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0f4bfbe4"},"source":["complete_word(\"tim\")"],"id":"0f4bfbe4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bd59b5b1"},"source":["complete_word(\"tha\")"],"id":"bd59b5b1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8a3fdb05"},"source":["complete_word(\"word\")"],"id":"8a3fdb05","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5f9e3a23"},"source":["## Document retrieval\n","\n","One of the most common uses of TF-IDF is information retrieval. A TF-IDF representation can be used to determine the similarity between different documents.\n","\n","Let's look at an example:"],"id":"5f9e3a23"},{"cell_type":"code","metadata":{"id":"494b96aa"},"source":["vect = TfidfVectorizer(\n","        sublinear_tf=True,\n","        min_df=0.025,\n","        max_df=0.9\n","        ).fit(df[\"text\"])"],"id":"494b96aa","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"26230725"},"source":["X = vect.transform(df[\"text\"]).toarray()\n","show_bow(X, vect.get_feature_names_out())"],"id":"26230725","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"80f19fcf"},"source":["We can calculate the most relevant documents in this way:"],"id":"80f19fcf"},{"cell_type":"code","metadata":{"id":"c9bb99b5"},"source":["query = \"there has been a delay since yesterday\"\n","q_repr = vect.transform([query]).toarray()\n","print(q_repr)"],"id":"c9bb99b5","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"57577699"},"source":["In this case, we use cosine similarity to determine the similarity of each document to the query:"],"id":"57577699"},{"cell_type":"code","metadata":{"id":"f75523ae"},"source":["sims = cosine_similarity(X, q_repr).flatten()\n","sims.shape"],"id":"f75523ae","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e152ce3a"},"source":["Finally, let's sort the documents in the corpus by similarity and display the five most similar ones."],"id":"e152ce3a"},{"cell_type":"code","metadata":{"id":"828b8868"},"source":["df[\"similarity\"] = sims\n","\n","for document in df.sort_values(\n","        by=\"similarity\",\n","        ascending=False\n","        ).iloc[:5, 0]:\n","    print(document)"],"id":"828b8868","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D1opaFLOw-KA"},"source":[],"id":"D1opaFLOw-KA","execution_count":null,"outputs":[]}]}